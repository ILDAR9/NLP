{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured prediction: PoS tagging Twitter\n",
    "### Ildar Nurgaliev (Innopolis university)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "import os\n",
    "get_full_path = lambda f_name : os.path.join(os.getcwd(), 'data', f_name)\n",
    "\n",
    "cng_postfix = lambda f_name, ext: f_name.rsplit( \".\", 1 )[ 0 ] + ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH[344]\t&lt; || 3 || : || 3 || Ugh || LOL || SMH || hey || aaahh || :( || XD || wait || :o || :D || :D || :D || Oh || :/ || &lt; || 3( || : || hahaha || aww || lol || Mmmm || Damn || Hey || there || LOL || btw || Amazing || yehhh || ( || : || Good || golly || good || morning || lol || ahhhh || =( || :) || Yeah || :' || Anytime || biotch || :) || haha || yes || :) || :) || xxx || right || lool || lol || Yes-sir || yep || btw || xD || lol || lol || yo || hey || :) || :) || lush || like || : || L || LMAO || ?: || P || :) || WTF || :D || &lt; || &lt; || 3 || yo || oh || =( || =) || alright || bro || ya || know....he || WELL || Okay || :' || ) || shit || :D || xHahaa || ;) || x || Morning || :( || hell || Lol || Yes || plz || :) || :) || o_O || yo || :( || (( || Yo || : || 3 || so || yeah~ || :D || OMG || yaaaayyy || ;) || smh || ok || Yes || ugh || so || :) || Hey || lmao || okay || :o || &lt;\n"
     ]
    }
   ],
   "source": [
    "def show_form(f_name, labels=None, size_show = 5):\n",
    "    fname = get_full_path(f_name)\n",
    "    res = []\n",
    "    content = []\n",
    "    with open(fname, 'r') as f:\n",
    "        content = f.readlines()\n",
    "    d = {}\n",
    "    for line in content:\n",
    "        line = line.strip() # remove newline from the end of the line\n",
    "    \n",
    "        if not len(line): #skip blank lines\n",
    "            continue\n",
    "\n",
    "        label, word = line.split(\"\\t\")\n",
    "        if labels:\n",
    "            if label in labels:\n",
    "                if label in d:\n",
    "                    d[label].append(word)\n",
    "                else:\n",
    "                   d[label] = [word] \n",
    "        else:\n",
    "            if label in d:\n",
    "                d[label].append(word)\n",
    "            else:\n",
    "               d[label] = [word]\n",
    "    for v in d:\n",
    "        print(\"%s[%d]\\t%s\" % (v, len(d[v]), \" || \".join(d[v][:size_show])))\n",
    "show_form('pos_train.conll', ('UH'), 127)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CD\t['2', '7', '2', '2010', '40s']\n",
      "RB\t['not', 'away', 'again', 'again', 'there']\n",
      "PDT\t['Half']\n",
      "NNP\t['Benitez', 'Soulja', 'Boy', 'TheDeAndreWay', 'DeAndre']\n",
      "JJ\t['4-1', 'New', 'ready', 'wet', 'cold']\n",
      "URL\t['http://youtu.be/pnhXhR07s14', 'com', 'souljaboytellem-iga.ning.com', 'http://bit.ly/fkdrr6', 'http://4sq.com/epmWHV']\n",
      "JJR\t['less', 'Less', 'More', 'better']\n",
      "EX\t['there', 'there', 'there', 'There']\n",
      "VBD\t['were', 'heard', 'came', 'tweeted', 'were']\n",
      "CC\t['But', 'and', 'and', 'and', 'and']\n",
      "PRP\t['I', 'you', 'it', 'it', 'you']\n",
      "VBZ\t['leads', 'scores', 'is', 'loves', '*kisses']\n",
      "PRP$\t['my', 'yo', 'your', 'my', 'my']\n",
      "WP\t['what', 'whts', 'WHAT', 'who', 'what']\n",
      "RBR\t['more', 'more', 'more', 'more', 'Earlier']\n",
      "TO\t['to', 'to', 'to', 'to', 'to']\n",
      ",\t[',', ',', ',', ',', ',']\n",
      "POS\t[\"'s\", \"'s\", \"'s\", \"'s\"]\n",
      "NONE\t['[', ']']\n",
      "WRB\t['when', 'where', 'when', 'When', 'WHEN']\n",
      "VBP\t['suppose', \"'m\", 'love', 'LUVZ', 'live']\n",
      ":\t[':', '|', '-', ':', ':']\n",
      "RP\t['ON', 'out', 'out', 'out', 'off']\n",
      "IN\t['til', 'in', 'for', 'after', 'than']\n",
      "USR\t['@jasonmunday', '@SouljaBoy', '@YnYrTheOne', '@Demi_Louis', '@malikismusic']\n",
      "DT\t['any', 'some', 'The', 'the', 'ALL']\n",
      "NN\t['case', 'JV', 'soccer', 'video', 'music']\n",
      ".\t['!', '?', '!', '.', '.']\n",
      "HT\t['#houseofmirrors', '#houseofmirrors', '#houseofmirrors', '#houseofmirrors', '#houseofmirrors']\n",
      "WDT\t['that']\n",
      "UH\t['Yay', 'awwwwwwww', ':)', 'Lol', 'So']\n",
      "NNS\t['days', 'days', 'mins', 'schedules', 'stores']\n",
      "VBG\t['tellin', 'stuntin', 'popping', 'cooling', 'Goin']\n",
      ")\t[')', ')', ')', ')', ')']\n",
      "''\t['\"', '\"', '\"', '\"', '\"']\n",
      "MD\t['will', 'will', 'can', 'will', 'will']\n",
      "JJS\t['Funniest', 'BEST', 'most']\n",
      "RT\t['RT', 'RT', 'RT', 'RT', 'RT']\n",
      "VB\t['let', 'play', 'go', 'RETWEET', 'get']\n",
      "SYM\t['=', '&lt;', '&gt;']\n",
      "(\t['(', '(', '(', '(']\n",
      "VBN\t['born', 'scared', 'reached', 'drafted', 'aka']\n"
     ]
    }
   ],
   "source": [
    "show_form('pos_test.conll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check labels of train and test datasets, outliers detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN/test:\n",
      "O[1]\t\"..\n",
      "FW[3]\tEtc || Etc || Etc\n",
      "VPP[1]\tplease\n",
      "NNPS[6]\tkids || Engineers || queens || Monsters || Eats\n",
      "TD[1]\ta\n",
      "RBS[1]\tmost\n",
      "LS[1]\t1\n",
      "\n",
      "TEST/train:\n",
      "PDT[1]\tHalf\n",
      "NONE[2]\t[ || ]\n",
      "\n",
      "Intersection: {'RBR', 'NN', ':', 'RB', 'HT', 'RT', '(', 'EX', 'VBD', 'CC', 'JJ', 'WRB', ',', 'WP', 'JJS', 'USR', ')', 'WDT', 'MD', 'TO', 'CD', 'DT', 'SYM', '.', 'JJR', 'RP', 'PRP$', 'IN', 'NNP', 'VB', 'VBP', 'UH', 'URL', \"''\", 'VBN', 'VBG', 'PRP', 'NNS', 'VBZ', 'POS'}\n"
     ]
    }
   ],
   "source": [
    "def get_labels(f_name):\n",
    "    fname = get_full_path(f_name)\n",
    "    res = []\n",
    "    content = []\n",
    "    s = set()\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip() # remove newline from the end of the line\n",
    "    \n",
    "            if not len(line): #skip blank lines\n",
    "                continue\n",
    "\n",
    "            label, _ = line.split(\"\\t\")\n",
    "            s.add(label)\n",
    "    return s\n",
    "        \n",
    "train_l = get_labels('pos_train.conll')\n",
    "test_l = get_labels('pos_test.conll')\n",
    "\n",
    "print(\"TRAIN/test:\")\n",
    "show_form('pos_train.conll', (train_l - test_l))\n",
    "print()\n",
    "print(\"TEST/train:\")\n",
    "show_form('pos_test.conll', (test_l - train_l))\n",
    "print()\n",
    "print(\"Intersection: %s\" % (train_l & test_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS taggins:\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "#### Train\n",
    "- ...\n",
    "- NNPS\tProper noun, plural\n",
    "\n",
    "#### Test\n",
    "- None\tnothing\n",
    "- PDT\tPredeterminer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sentences and tag its words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_train.conll 551 sentences\n",
      "['Antick_NNP', 'Musings_NNP', 'post_NN', ':_:', 'Book-A-Day_NNP', '2010_CD', '#_NN', '243_CD', '(_(', '10/4_CD', ')_)', '--_:', 'Gray_NNP', 'Horses_NNP', 'by_IN', 'Hope_NNP', 'Larson_NNP', 'http://bit.ly/as8fvc_URL']\n"
     ]
    }
   ],
   "source": [
    "def get_sentences(f_name):\n",
    "    fname = get_full_path(f_name)\n",
    "    rows = []\n",
    "    with open(fname, 'r') as f:\n",
    "        rows = f.readlines()\n",
    "    sents = []\n",
    "    sent = []\n",
    "    for row in rows:\n",
    "        row = row.strip()\n",
    "        if row == '':\n",
    "            if len(sent) > 0:\n",
    "                sents.append(sent)\n",
    "            sent = []\n",
    "            continue\n",
    "        pos, word = row.split('\\t',1)\n",
    "        sent.append(word + \"_\" + pos)\n",
    "    print(f_name, len(sents), 'sentences')\n",
    "    return sents\n",
    "for x in get_sentences('pos_train.conll')[:1]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "words = set(words.words())\n",
    "def word_features(sent, iw):\n",
    "    try:\n",
    "        word, _ = sent[iw].rsplit( \"_\", 1 )\n",
    "    except ValueError as v:\n",
    "        print(sent[iw])\n",
    "#   feature functions\n",
    "    is_hashtag = lambda x : x[0] == '#'\n",
    "    is_user = lambda x : x[0] == '@'\n",
    "    is_url = lambda x : 'http' in x or 'com' in x\n",
    "    is_upper = lambda x : x[0].isupper()\n",
    "    is_digit = lambda x : x[0].isdigit()\n",
    "    is_word = lambda x : x.lower() in words\n",
    "#     pos = lambda x : \n",
    "    def pos(x):\n",
    "        p = lesk(sent, x.lower())\n",
    "        if p:\n",
    "            return p.pos()\n",
    "        else:\n",
    "            \"0\"\n",
    "    \n",
    "    def num_same_elemseq(x):\n",
    "        el = len(set(x))\n",
    "        if el == 1:\n",
    "            return \"1\"\n",
    "        elif el == 2:\n",
    "            return \"2\"\n",
    "        elif el == 0:\n",
    "            return \"0\"\n",
    "        else:\n",
    "            return \"+\"\n",
    "    def rev_type(x):\n",
    "        if is_upper(x):\n",
    "            return \"1\"\n",
    "        elif is_digit(x):\n",
    "            return \"2\"\n",
    "        elif is_user(x):\n",
    "            return \"3\"\n",
    "        elif is_hashtag(x):\n",
    "            return \"4\"\n",
    "        elif is_url(x):\n",
    "            return \"5\"\n",
    "        else:\n",
    "            return \"0\"\n",
    "        \n",
    "        \n",
    "#     word features\n",
    "    features = [\n",
    "        'w.lower=' + word.lower(),\n",
    "        'w.len=%s' % len(word),\n",
    "        'w.last=' + word[-1],\n",
    "        'w.sameseq=%s' % num_same_elemseq(word),\n",
    "        'w.isupper=%s' % is_upper(word),\n",
    "        'w.type=%s' % rev_type(word),\n",
    "        'w.iw=%s' % iw\n",
    "                \n",
    "    ]\n",
    "    if (is_word(word)):\n",
    "        features.extend([\n",
    "                'w.isword=True',\n",
    "                'w[-2]=' + word[-2:],\n",
    "#                 'w[-3]=' + word[-3:],\n",
    "            ])\n",
    "    else:\n",
    "        features.append('w.isword=False')\n",
    "        \n",
    "#   relative words features (PRE)\n",
    "    if iw > 0:\n",
    "        pre_word = sent[iw - 1][0]\n",
    "        features.extend([\n",
    "            '-w.lower=' + pre_word.lower(),\n",
    "            '-w.len=%s' % len(pre_word),\n",
    "            '-w.isupper=%s' % is_upper(pre_word),\n",
    "            '-w[-1]=' + pre_word[-1],\n",
    "            '-w.sameseq=%s' % num_same_elemseq(pre_word),\n",
    "            '-w.type=%s' % rev_type(pre_word)\n",
    "        ])\n",
    "        \n",
    "        if (is_word(pre_word)):\n",
    "            features.extend([\n",
    "                '-w.isword=True',\n",
    "                '-w[-2]=' + pre_word[-2:],\n",
    "                '-w.pos=%s' % pos(pre_word)\n",
    "            ])\n",
    "        else:\n",
    "            features.append('w.isword=False')\n",
    "        \n",
    "    else:\n",
    "        features.append('F:BoS')\n",
    "#   relative words features (POST)\n",
    "    if iw < len(sent)-1:\n",
    "        post_word = sent[iw + 1][0]\n",
    "        features.extend([\n",
    "            '+w.lower=' + post_word.lower(),\n",
    "            '+w.len=%s' % len(post_word),\n",
    "            '+w.isupper=%s' % is_upper(post_word),\n",
    "            '+w.last=%s' % post_word[-1],\n",
    "            '+w.sameseq=%s' % num_same_elemseq(post_word),\n",
    "            '+w.type=%s' % rev_type(post_word)\n",
    "        ])\n",
    "        \n",
    "        if (is_word(post_word)):\n",
    "            features.extend([\n",
    "                '+w.isword=True',\n",
    "                '+w[-2]=' + post_word[-2:],\n",
    "                '+w.pos=%s' % pos(post_word)\n",
    "            ])\n",
    "        else:\n",
    "            features.append('+w.isword=False')\n",
    "        \n",
    "    else:\n",
    "        features.append('F:EoS')\n",
    "   \n",
    "                \n",
    "    return features\n",
    "\n",
    "sent_word_features=lambda sent:[word_features(sent, i) for i in range(len(sent))]\n",
    "sent_word_labels=lambda sent: [word_tag.rsplit('_',1)[1] for word_tag in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_train.conll 551 sentences\n",
      "saved to pos_train.input\n",
      "pos_test.conll 118 sentences\n",
      "saved to pos_test.input\n",
      "CPU times: user 1.62 s, sys: 36.3 ms, total: 1.65 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outliers_tag = set(('LS',\n",
    "    'TD',\n",
    "    'FW',\n",
    "    'NNPS',\n",
    "    'VPP',\n",
    "    'RBS',\n",
    "    'O'))\n",
    "def filter_tag(x):\n",
    "    return x not in outliers_tag\n",
    "\n",
    "def features_preserve(f_name):\n",
    "    sents = get_sentences(f_name)\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        tags = sent_word_labels(sent)\n",
    "        features = sent_word_features(sent)\n",
    "        for label, crfsuite_features in zip(tags, features):\n",
    "            if filter_tag(label):\n",
    "                res.append(label + \"\\t\" + \"\\t\".join(crfsuite_features))\n",
    "        res.append('\\n')\n",
    "    # output file\n",
    "    f_name = cng_postfix(f_name, '.input')\n",
    "    fname = get_full_path(f_name)\n",
    "    # preserve data\n",
    "    with open(fname, 'w') as f:\n",
    "        for word in res:\n",
    "            f.write(word)\n",
    "            f.write('\\n')\n",
    "    print(\"saved to\", f_name)\n",
    "            \n",
    "features_preserve('pos_train.conll')\n",
    "features_preserve('pos_test.conll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.26 ms, sys: 13.1 ms, total: 16.4 ms\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from subprocess import check_output, call\n",
    "\n",
    "fmodel='my1.model'\n",
    "ftrain='data/pos_train.input'\n",
    "ftest='data/pos_test.input'\n",
    "\n",
    "algs = (\n",
    "    'lbfgs'                # L-BFGS with L1/L2 regularization\n",
    "    'l2sgd'                # SGD with L2-regularization\n",
    "    'ap'                   # Averaged Perceptron\n",
    "    'pa'                   # Passive Aggressive\n",
    "    'arow'                 # Adaptive Regularization of Weights (AROW)\n",
    "    )\n",
    "\n",
    "# res = call(['crfsuite', 'learn', '-m', fmodel, -p',  'feature.possible_states=1',ftrain])\n",
    "# res = call(['crfsuite', 'learn', '-m', fmodel,'-p', 'feature.possible_states=1', \n",
    "#              '-p', 'feature.minfreq=2', '-p', 'feature.possible_transitions=1', '-p', 'feature.minfreq=2', ftrain])\n",
    "\n",
    "# res = call(['crfsuite', 'learn', '-m', fmodel, '-a', 'lbfgs', '-p', 'c1=0.1', '-p', 'c2=0.01',\n",
    "#   '-p', 'feature.possible_transitions=1', ftrain])\n",
    "# res = call(['crfsuite', 'learn', '-m', fmodel, '-a', 'lbfgs', '-p', 'linesearch=StrongBacktracking', #MoreThuente Backtracking StrongBacktracking\n",
    "#             '-p', 'c1=0.1', '-p', 'c2=0.01', ftrain])\n",
    "\n",
    "# res = call(['crfsuite', 'learn', '-m', fmodel, '-a', 'l2sgd', '-p', 'c2=0.01', ftrain])\n",
    "\n",
    "# res = call(['crfsuite', 'learn', '-m', fmodel, '-a', 'ap', '-p', 'max_iterations=150', ftrain])\n",
    "#                                                                 0 1 2\n",
    "res = call(['crfsuite', 'learn', '-m', fmodel, '-a', 'pa', '-p', 'type=1', '-p', 'c=0.1', '-p', 'max_iterations=300',\n",
    "            '-p', 'error_sensitive=1', '-p', 'averaging=1', '-p', 'feature.possible_transitions=1', ftrain])\n",
    "\n",
    "# res = call(['crfsuite', 'learn', '-m', fmodel, '-a', 'arow', '-p', 'max_iterations=200',\n",
    "#             '-p', 'gamma=0.1', '-p', 'variance=0.08', ftrain])\n",
    "\n",
    "\n",
    "if res == 0:\n",
    "    out = check_output(['crfsuite', 'dump', fmodel, '|', 'less'])\n",
    "#     print(out.decode())\n",
    "else:\n",
    "    print('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my10.model\n",
      "Performance by label (#match, #model, #ref) (precision, recall, F1):\n",
      "    NNP: (133, 200, 169) (0.6650, 0.7870, 0.7209)\n",
      "    NN: (227, 280, 286) (0.8107, 0.7937, 0.8021)\n",
      "    : (77, 82, 81) (0.9390, 0.9506, 0.9448)\n",
      "    CD: (33, 40, 35) (0.8250, 0.9429, 0.8800)\n",
      "    (: (4, 4, 4) (1.0000, 1.0000, 1.0000)\n",
      "    ): (5, 5, 6) (1.0000, 0.8333, 0.9091)\n",
      "    IN: (158, 171, 167) (0.9240, 0.9461, 0.9349)\n",
      "    URL: (22, 24, 23) (0.9167, 0.9565, 0.9362)\n",
      "    RT: (24, 24, 24) (1.0000, 1.0000, 1.0000)\n",
      "    USR: (63, 63, 64) (1.0000, 0.9844, 0.9921)\n",
      "    HT: (26, 26, 26) (1.0000, 1.0000, 1.0000)\n",
      "    .: (122, 125, 123) (0.9760, 0.9919, 0.9839)\n",
      "    WRB: (22, 23, 22) (0.9565, 1.0000, 0.9778)\n",
      "    PRP: (158, 161, 164) (0.9814, 0.9634, 0.9723)\n",
      "    VBP: (68, 86, 82) (0.7907, 0.8293, 0.8095)\n",
      "    MD: (27, 28, 27) (0.9643, 1.0000, 0.9818)\n",
      "    RB: (83, 100, 92) (0.8300, 0.9022, 0.8646)\n",
      "    VB: (67, 86, 89) (0.7791, 0.7528, 0.7657)\n",
      "    UH: (42, 61, 63) (0.6885, 0.6667, 0.6774)\n",
      "    VBG: (32, 40, 35) (0.8000, 0.9143, 0.8533)\n",
      "    JJ: (71, 90, 116) (0.7889, 0.6121, 0.6893)\n",
      "    ,: (40, 40, 40) (1.0000, 1.0000, 1.0000)\n",
      "    CC: (42, 45, 43) (0.9333, 0.9767, 0.9545)\n",
      "    PRP$: (27, 30, 32) (0.9000, 0.8438, 0.8710)\n",
      "    DT: (104, 108, 115) (0.9630, 0.9043, 0.9327)\n",
      "    JJS: (1, 2, 3) (0.5000, 0.3333, 0.4000)\n",
      "    NNS: (53, 67, 64) (0.7910, 0.8281, 0.8092)\n",
      "    VBZ: (48, 57, 58) (0.8421, 0.8276, 0.8348)\n",
      "    RBR: (4, 6, 6) (0.6667, 0.6667, 0.6667)\n",
      "    VBN: (10, 20, 18) (0.5000, 0.5556, 0.5263)\n",
      "    VBD: (34, 39, 44) (0.8718, 0.7727, 0.8193)\n",
      "    TO: (40, 41, 42) (0.9756, 0.9524, 0.9639)\n",
      "    RP: (10, 11, 14) (0.9091, 0.7143, 0.8000)\n",
      "    EX: (0, 0, 4) (0.0000, 0.0000, 0.0000)\n",
      "    POS: (3, 3, 4) (1.0000, 0.7500, 0.8571)\n",
      "    WP: (9, 9, 10) (1.0000, 0.9000, 0.9474)\n",
      "    WDT: (1, 1, 1) (1.0000, 1.0000, 1.0000)\n",
      "    JJR: (1, 2, 4) (0.5000, 0.2500, 0.3333)\n",
      "    '': (10, 11, 10) (0.9091, 1.0000, 0.9524)\n",
      "    SYM: (0, 0, 1) (0.0000, 0.0000, 0.0000)\n",
      "Macro-average precision, recall, F1: (0.822436, 0.802564, 0.809107)\n",
      "Item accuracy: 1901 / 2211 (0.8598)\n",
      "Instance accuracy: 21 / 116 (0.1810)\n",
      "Elapsed time: 0.045958 [sec] (2567.6 [instance/sec])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(fmodel)\n",
    "out = check_output(['crfsuite', 'tag', '-q', '-m',fmodel, '-t' ,ftest])\n",
    "print(out.decode())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ds]",
   "language": "python",
   "name": "conda-env-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
