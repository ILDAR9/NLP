{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Asignment**: Senseval-2 system. Due October 7.\n",
    "1. Describe your method\n",
    "* Analyse the errors\n",
    "* Include data and examples\n",
    "* Suggest how to improve the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HARD1', 'HARD2', 'HARD3'}\n",
      "{'hard-a'}\n",
      "HARD1 the slump in business activity and the accompanying `` credit crunch '' have been especially hard on owners of these high-yielding , high-risk securities , whether they owned them directly or through mutual funds .\n",
      "HARD2 he said tuesday there are no hard feelings .\n",
      "HARD3 `` it 's very difficult to sell women 's apparel in a store whose reputation is hard goods , '' said monroe greenstein , an analyst at bear , stearns & co .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import senseval as se\n",
    "\n",
    "x = se.instances(\"hard.pos\")\n",
    "ex = x[20:220]+x[3500:3550]+x[4100:4150]    #subset of 300 examples\n",
    "s = set()\n",
    "w = set()\n",
    "f_text = lambda s : \" \".join(next(zip(*s.context)))\n",
    "for sent in ex:\n",
    "    sense = sent.senses[0]\n",
    "    s.add(sense)\n",
    "    w.add(sent.word)\n",
    "\n",
    "print(s)\n",
    "print(w)\n",
    "print(x[20].senses[0], f_text(x[20]))\n",
    "print(x[3500].senses[0], f_text(x[3500]))\n",
    "print(x[4100].senses[0], f_text(x[4100]))\n",
    "del(s, x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARD3 hard-a `` it 's very difficult to sell women 's apparel in a store whose reputation is hard goods , '' said monroe greenstein , an analyst at bear , stearns & co .\n"
     ]
    }
   ],
   "source": [
    "k = 250\n",
    "print(ex[k].senses[0], ex[k].word, f_text(ex[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difficult.a.01\n",
      "not easy; requiring great physical or mental effort to accomplish or comprehend or endure\n",
      "hard.a.02\n",
      "dispassionate; \n",
      "hard.a.03\n",
      "resisting weight or pressure\n",
      "hard.s.04\n",
      "very strong or vigorous\n",
      "arduous.s.01\n",
      "characterized by effort to the point of exhaustion; especially physical effort\n",
      "unvoiced.a.01\n",
      "produced without vibration of the vocal cords\n",
      "hard.a.07\n",
      "(of light) transmitted directly from a pointed light source\n",
      "hard.a.08\n",
      "(of speech sounds); produced with the back of the tongue raised toward or touching the velum\n",
      "intemperate.s.03\n",
      "given to excessive indulgence of bodily appetites especially for intoxicating liquors\n",
      "hard.s.10\n",
      "being distilled rather than fermented; having a high alcoholic content\n",
      "hard.s.11\n",
      "unfortunate or hard to bear\n",
      "hard.s.12\n",
      "dried out\n",
      "hard.r.01\n",
      "with effort or force or vigor\n",
      "hard.r.02\n",
      "with firmness\n",
      "hard.r.03\n",
      "earnestly or intently\n",
      "hard.r.04\n",
      "causing great damage or hardship\n",
      "hard.r.05\n",
      "slowly and with difficulty\n",
      "heavily.r.07\n",
      "indulging excessively\n",
      "hard.r.07\n",
      "into a solid condition\n",
      "hard.r.08\n",
      "very near or close in space or time\n",
      "hard.r.09\n",
      "with pain or distress or bitterness\n",
      "hard.r.10\n",
      "to the full extent possible; all the way\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wd\n",
    "\n",
    "word = \"hard\"\n",
    "w = wd.morphy(word)\n",
    "if w:\n",
    "  lemmas = wd.lemmas(w)\n",
    "  for l in lemmas:\n",
    "    syn = l.synset()\n",
    "    print(syn.name())\n",
    "    print(syn.definition())\n",
    "    print\n",
    "else:\n",
    "  print(\"WordNet has no base form for\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# syn_key = 'hard.a.07'\n",
    "# syn_key = 'unvoiced.a.01'\n",
    "# syn_key = 'hard.s.11'\n",
    "syn_key = 'hard.a.03'\n",
    "s = wd.synset(syn_key)\n",
    "print(s.definition())\n",
    "print(s.examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clear_sent(sentence):\n",
    "    # Stop words\n",
    "    unwanted = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    # Symbols\n",
    "    unwanted.update(list('!\"#$%&\\'()*+,-./:;<=>? @[\\\\]^_`{|}~Â£'))\n",
    "    unwanted.update([\"''\"])\n",
    "    unwanted.update([\"``\"])\n",
    "    unwanted.update(['\"\"'])\n",
    "\n",
    "    toks = [w.lower() for w in (sentence if isinstance(sentence, list) \\\n",
    "                    else sentence.split()) if w not in unwanted]\n",
    "    return toks  \n",
    "\n",
    "SENSE_MAP = {\n",
    "    \"HARD1\": [\"difficult.a.01\", \"hard.s.11\"],\n",
    "    \"HARD2\": [\"hard.a.02\", \"difficult.a.01\"],\n",
    "    \"HARD3\": [\"hard.a.03\"],   \n",
    "}\n",
    "def from_eval_to_sense(s):\n",
    "    if s in SENSE_MAP:\n",
    "        return SENSE_MAP[s]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised algortihms evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little test of lesk usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----10\n",
      "`` i have a hard time with the use of the word . ''\n",
      "_real: ('HARD1',) - ['difficult.a.01', 'hard.s.11']\n",
      " with full: Synset('hard.a.07')\n",
      " with filtered: Synset('unvoiced.a.01')\n",
      "-----11\n",
      "`` when you have a disaster like the oakland hills fire , it 's hard to remember everything .\n",
      "_real: ('HARD1',) - ['difficult.a.01', 'hard.s.11']\n",
      " with full: Synset('hard.a.07')\n",
      " with filtered: Synset('unvoiced.a.01')\n",
      "-----12\n",
      "`` it was a unique project , a high-risk project , and it was put in such an unfortunate time slot ( opposite '48 hours ' and 'quantum leap ' ) that it would be hard to get anyone interested in it .\n",
      "_real: ('HARD1',) - ['difficult.a.01', 'hard.s.11']\n",
      " with full: Synset('hard.a.07')\n",
      " with filtered: Synset('unvoiced.a.01')\n",
      "-----13\n",
      "`` four generations of our family have come here , and every year , it gets harder to get in . ''\n",
      "_real: ('HARD1',) - ['difficult.a.01', 'hard.s.11']\n",
      " with full: Synset('hard.a.07')\n",
      " with filtered: Synset('unvoiced.a.01')\n",
      "-----14\n",
      "all the easy cuts and a bunch of the hard ones were made in bridging the $ 14 billion gap in june .\n",
      "_real: ('HARD1',) - ['difficult.a.01', 'hard.s.11']\n",
      " with full: Synset('hard.a.07')\n",
      " with filtered: Synset('unvoiced.a.01')\n"
     ]
    }
   ],
   "source": [
    "def test(c):\n",
    "    context = next(zip(*c.context))\n",
    "    s = \" \".join(context)\n",
    "    print(s)\n",
    "    print(\"_real: %s - %s\\n with full: %s\\n with filtered: %s\" % \n",
    "      (c.senses, from_eval_to_sense(c.senses[0]),\n",
    "       lesk(s, 'hard',pos='a'), lesk(clear_sent(s), 'hard',pos='a')))\n",
    "    \n",
    "k = 10\n",
    "for i in range(k,k+5):\n",
    "    print('-----%d' % i)\n",
    "    test(ex[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesk improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import wordnet as wd\n",
    "\n",
    "# as more as we could google theme\n",
    "functionwords = ['about', 'across', 'against', 'along', 'around', 'at',\n",
    "                 'behind', 'beside', 'besides', 'by', 'despite', 'down',\n",
    "                 'during', 'for', 'from', 'in', 'inside', 'into', 'near', 'of',\n",
    "                 'off', 'on', 'onto', 'over', 'through', 'to', 'toward',\n",
    "                 'with', 'within', 'without', 'anything', 'everything',\n",
    "                 'anyone', 'everyone', 'ones', 'such', 'it', 'itself',\n",
    "                 'something', 'nothing', 'someone', 'the', 'some', 'this',\n",
    "                 'that', 'every', 'all', 'both', 'one', 'first', 'other',\n",
    "                 'next', 'many', 'much', 'more', 'most', 'several', 'no', 'a',\n",
    "                 'an', 'any', 'each', 'no', 'half', 'twice', 'two', 'second',\n",
    "                 'another', 'last', 'few', 'little', 'less', 'least', 'own',\n",
    "                 'and', 'but', 'after', 'when', 'as', 'because', 'if', 'what',\n",
    "                 'where', 'which', 'how', 'than', 'or', 'so', 'before', 'since',\n",
    "                 'while', 'although', 'though', 'who', 'whose', 'can', 'may',\n",
    "                 'will', 'shall', 'could', 'be', 'do', 'have', 'might', 'would',\n",
    "                 'should', 'must', 'here', 'there', 'now', 'then', 'always',\n",
    "                 'never', 'sometimes', 'usually', 'often', 'therefore',\n",
    "                 'however', 'besides', 'moreover', 'though', 'otherwise',\n",
    "                 'else', 'instead', 'anyway', 'incidentally', 'meanwhile']\n",
    "\n",
    "def overlapcontext( synset, sentence ):\n",
    "    gloss = TreebankWordTokenizer().tokenize(synset.definition())\n",
    "    gloss = set(clear_sent(gloss))\n",
    "    for i in synset.examples():\n",
    "         gloss.union(i)\n",
    "    gloss = gloss.difference( functionwords )\n",
    "    if isinstance(sentence, list):\n",
    "        sentence = set(sentence)\n",
    "    else:\n",
    "        return\n",
    "    sentence = sentence.difference( functionwords )\n",
    "    return len( gloss.intersection(sentence) )\n",
    "\n",
    "def lesk2( sentence, word, pos = None):\n",
    "    bestsense = None\n",
    "    maxoverlap = 0\n",
    "    word=wd.morphy(word, pos=pos) if wd.morphy(word) is not None else word\n",
    "    for sense in wd.synsets(word, pos=pos):\n",
    "        overlap = overlapcontext(sense,sentence)\n",
    "        for h in sense.hyponyms():\n",
    "            overlap += overlapcontext( h, sentence )\n",
    "        if overlap > maxoverlap:\n",
    "                maxoverlap = overlap\n",
    "                bestsense = sense\n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little test of lesk updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test2(c):\n",
    "    context = next(zip(*c.context))\n",
    "    s = \" \".join(context)\n",
    "    print(s)\n",
    "    print(\"_real: %s - %s\\n with full: %s\\n with filtered: %s\" % \n",
    "      (c.senses, from_eval_to_sense(c.senses[0]),\n",
    "       lesk2('hard',s), lesk2('hard', clear_sent(s))))\n",
    "    \n",
    "k = 260\n",
    "for i in range(k,k+5):\n",
    "    print('-----%d' % i)\n",
    "    test2(ex[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual check lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "sentence_input = widgets.Text(description=\"sentence\")\n",
    "word_input = widgets.Text(description=\"word\")\n",
    "\n",
    "display(sentence_input)\n",
    "display(word_input)\n",
    "\n",
    "\n",
    "btn = widgets.Button(description=\"submit text inputs\")\n",
    "display(btn)\n",
    "\n",
    "def on_btn_click(e):\n",
    "    word = word_input.value\n",
    "    sentence = sentence_input.value\n",
    "    \n",
    "    sentence = TreebankWordTokenizer().tokenize(sentence)\n",
    "    sentence = clear_sent(sentence)\n",
    "    print(word, sentence)\n",
    "    res = lesk2(word, sentence)\n",
    "    print(res)\n",
    "    \n",
    "btn.on_click(on_btn_click)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated test of lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.0\n",
      "Recall = 0.0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f41484b55de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1-score =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "def clarify(x):\n",
    "    s = f_text(x)\n",
    "    s = clear_sent(s)\n",
    "    s = \n",
    "#     return lesk2(s, 'hard', pos='a')\n",
    "    return lesk(s, 'hard', pos='a')\n",
    "    \n",
    "res = []\n",
    "for x in ex:\n",
    "    y1 = clarify(x)\n",
    "    y = from_eval_to_sense(x.senses[0])\n",
    "    if y1:\n",
    "        r = y1.name() in y\n",
    "        res.append(r)\n",
    "#         print(r,y1.name(),y)\n",
    "#     else:\n",
    "#         print(y1)\n",
    "\n",
    "if True:\n",
    "    t = len(list(filter(lambda x : x, res)))\n",
    "    f = len(res) - t\n",
    "    n = len(ex)\n",
    "    prec = t/len(res)\n",
    "    rec = t/n\n",
    "    print(\"Precision =\", prec)\n",
    "    print(\"Recall =\", rec)\n",
    "    print(\"F1-score =\", prec*rec/(prec+rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Lesk\n",
    "\n",
    "#### pos=None\n",
    "- Precision = 0.5066666666666667\n",
    "- Recall = 0.5066666666666667\n",
    "- F1-score = 0.25333333333333335\n",
    "\n",
    "#### pos='a'\n",
    "- Precision = 0.0033333333333333335\n",
    "- Recall = 0.0033333333333333335\n",
    "- F1-score = 0.001666666666666667\n",
    "\n",
    "### Lesk2\n",
    "\n",
    "#### pos=None\n",
    "- Precision = 0.5559701492537313\n",
    "- Recall = 0.49666666666666665\n",
    "- F1-score = 0.2623239436619718\n",
    "\n",
    "#### pos='a'\n",
    "- Precision = 0.5665399239543726\n",
    "- Recall = 0.49666666666666665\n",
    "- F1-score = 0.26465364120781526"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ds]",
   "language": "python",
   "name": "conda-env-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0c73ef7c2cc543759b3e0e07bd5f29c5": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "1c5db983c6334c608385b6103e7ce736": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "e60b30824f6d4f8386534fb7db787f24": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
